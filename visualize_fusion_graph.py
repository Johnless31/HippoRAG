#!/usr/bin/env python3
"""
èåˆå¯è§†åŒ–HippoRAGå›¾è°± - ç»“åˆgraph.pickleå’ŒOpenIEæ•°æ®
å±•ç¤ºæœ€çœŸå®ã€æœ€ä¸°å¯Œçš„çŸ¥è¯†å›¾è°±ä¿¡æ¯
"""

import pickle
import json
import os
from pyvis.network import Network
import networkx as nx
from collections import defaultdict, Counter

def load_hipporag_graph(pickle_file):
    """åŠ è½½HippoRAGçš„igraphå›¾è°±"""
    try:
        import igraph as ig
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…igraph: pip install igraph")
        return None
    
    if not os.path.exists(pickle_file):
        print(f"âŒ æ‰¾ä¸åˆ°å›¾è°±æ–‡ä»¶: {pickle_file}")
        return None
    
    with open(pickle_file, 'rb') as f:
        igraph_obj = pickle.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½HippoRAGå›¾è°±: {igraph_obj.summary()}")
    return igraph_obj

def load_openie_data(json_file):
    """åŠ è½½OpenIEç»“æœæ•°æ®"""
    if not os.path.exists(json_file):
        print(f"âŒ æ‰¾ä¸åˆ°OpenIEæ–‡ä»¶: {json_file}")
        return None
    
    with open(json_file, 'r', encoding='utf-8') as f:
        openie_data = json.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½OpenIEæ•°æ®: {len(openie_data['docs'])} ä¸ªæ–‡æ¡£")
    return openie_data

def extract_openie_semantics(openie_data):
    """ä»OpenIEæ•°æ®ä¸­æå–è¯­ä¹‰ä¿¡æ¯"""
    entity_contexts = defaultdict(list)
    entity_relations = defaultdict(list)
    entity_types = defaultdict(set)
    relation_mappings = defaultdict(list)
    
    # ç»Ÿè®¡å®ä½“é¢‘ç‡ï¼ˆåŒ…æ‹¬ä¸‰å…ƒç»„ä¸­çš„æ¦‚å¿µï¼‰
    entity_frequencies = Counter()
    
    for doc in openie_data['docs']:
        passage = doc['passage']
        entities = doc['extracted_entities']
        triples = doc['extracted_triples']
        
        # æ”¶é›†æ‰€æœ‰å®ä½“
        all_entities = set(entities)
        for triple in triples:
            subject, relation, obj = triple
            all_entities.add(subject)
            all_entities.add(obj)
        
        # è®°å½•ä¸Šä¸‹æ–‡
        for entity in all_entities:
            entity_contexts[entity].append(passage)
            entity_frequencies[entity] += 1
        
        # è®°å½•å…³ç³»
        for triple in triples:
            subject, relation, obj = triple
            relation_str = f"{subject} â†’ {relation} â†’ {obj}"
            entity_relations[subject].append(relation_str)
            entity_relations[obj].append(relation_str)
            relation_mappings[(subject, obj)].append((relation, passage))
            relation_mappings[(obj, subject)].append((relation, passage))  # åŒå‘è®°å½•
    
    # æ¨æ–­å®ä½“ç±»å‹
    for entity in entity_frequencies:
        relations = entity_relations[entity]
        inferred_types = infer_entity_type(entity, relations)
        entity_types[entity].update(inferred_types)
    
    return entity_contexts, entity_relations, entity_types, entity_frequencies, relation_mappings

def infer_entity_type(entity_name, relations):
    """ä»å…³ç³»ä¸­æ¨æ–­å®ä½“ç±»å‹"""
    entity_types = set()
    
    for relation_str in relations:
        if "is a" in relation_str:
            parts = relation_str.split(" â†’ ")
            if len(parts) == 3 and parts[1] == "is a" and parts[0] == entity_name:
                entity_types.add(parts[2])
        elif "born in" in relation_str or "birthplace" in relation_str:
            if entity_name in relation_str.split(" â†’ ")[0]:
                entity_types.add("Person")
            else:
                entity_types.add("Place")
        elif "is part of" in relation_str:
            if entity_name in relation_str.split(" â†’ ")[0]:
                entity_types.add("Place/Region")
            else:
                entity_types.add("Larger Region")
    
    # æ ¹æ®å®ä½“åç§°æ¨æ–­
    if "County" in entity_name:
        entity_types.add("Administrative Region")
    elif entity_name in ["Cinderella", "prince"]:
        entity_types.add("Character")
    elif entity_name in ["kingdom", "royal ball"]:
        entity_types.add("Place/Event")
    elif "slipper" in entity_name:
        entity_types.add("Object")
    elif entity_name == "politician":
        entity_types.add("Profession/Role")
    
    return list(entity_types)

def igraph_to_networkx_with_semantics(igraph_obj, openie_semantics):
    """å°†igraphè½¬æ¢ä¸ºNetworkXå¹¶èåˆè¯­ä¹‰ä¿¡æ¯"""
    entity_contexts, entity_relations, entity_types, entity_frequencies, relation_mappings = openie_semantics
    
    # åˆ›å»ºNetworkXå›¾
    G = nx.Graph()
    
    # è¯Šæ–­ä¿¡æ¯
    missing_entities = []
    matched_entities = []
    
    # æ·»åŠ èŠ‚ç‚¹å¹¶èåˆè¯­ä¹‰ä¿¡æ¯
    for i in range(igraph_obj.vcount()):
        node_attrs = {}
        for attr in igraph_obj.vertex_attributes():
            node_attrs[attr] = igraph_obj.vs[i][attr]
        
        node_id = node_attrs.get('name', f'node_{i}')
        node_content = node_attrs.get('content', '').lower()
        
        # èåˆOpenIEè¯­ä¹‰ä¿¡æ¯
        if node_id.startswith('entity-'):
            # æŸ¥æ‰¾å¯¹åº”çš„å®ä½“åç§°
            matching_entity = None
            for entity in entity_frequencies:
                if entity.lower() == node_content:
                    matching_entity = entity
                    break
            
            if matching_entity:
                matched_entities.append((node_id, matching_entity, node_content))
                node_attrs['semantic_name'] = matching_entity
                node_attrs['contexts'] = entity_contexts[matching_entity]
                node_attrs['relations'] = entity_relations[matching_entity]
                node_attrs['inferred_types'] = list(entity_types[matching_entity])
                node_attrs['frequency'] = entity_frequencies[matching_entity]
            else:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­æ–‡å®ä½“è¢«é”™è¯¯å¤„ç†
                is_likely_chinese = node_content == '' and node_id.startswith('entity-')
                if is_likely_chinese:
                    missing_entities.append((node_id, node_content, "å¯èƒ½æ˜¯ä¸­æ–‡å®ä½“è¢«é”™è¯¯å¤„ç†"))
                else:
                    missing_entities.append((node_id, node_content, "æœªçŸ¥åŸå› "))
                
                node_attrs['semantic_name'] = node_content if node_content else "æœªçŸ¥å®ä½“"
                node_attrs['contexts'] = []
                node_attrs['relations'] = []
                node_attrs['inferred_types'] = []
                node_attrs['frequency'] = 0
        
        G.add_node(node_id, **node_attrs)
    
    # æ·»åŠ è¾¹å¹¶èåˆè¯­ä¹‰ä¿¡æ¯
    for i in range(igraph_obj.ecount()):
        edge = igraph_obj.es[i]
        source_idx = edge.source
        target_idx = edge.target
        
        source_name = igraph_obj.vs[source_idx]['name']
        target_name = igraph_obj.vs[target_idx]['name']
        
        edge_attrs = {}
        for attr in igraph_obj.edge_attributes():
            edge_attrs[attr] = edge[attr]
        
        # èåˆè¯­ä¹‰å…³ç³»ä¿¡æ¯
        source_content = igraph_obj.vs[source_idx]['content']
        target_content = igraph_obj.vs[target_idx]['content']
        
        # æŸ¥æ‰¾è¯­ä¹‰å…³ç³»
        semantic_relations = []
        if (source_content, target_content) in relation_mappings:
            semantic_relations.extend(relation_mappings[(source_content, target_content)])
        if (target_content, source_content) in relation_mappings:
            semantic_relations.extend(relation_mappings[(target_content, source_content)])
        
        edge_attrs['semantic_relations'] = semantic_relations
        
        G.add_edge(source_name, target_name, **edge_attrs)
    
    # æ‰“å°è¯Šæ–­ä¿¡æ¯
    print(f"\nğŸ” å®ä½“åŒ¹é…è¯Šæ–­:")
    print(f"   âœ… æˆåŠŸåŒ¹é…çš„å®ä½“: {len(matched_entities)}")
    for node_id, semantic_name, graph_content in matched_entities:
        print(f"     - {node_id[:20]}... -> '{semantic_name}' (å›¾è°±: '{graph_content}')")
    
    if missing_entities:
        print(f"   âŒ ç¼ºå¤±çš„å®ä½“: {len(missing_entities)}")
        for node_id, graph_content, reason in missing_entities:
            print(f"     - {node_id[:20]}... -> '{graph_content}' ({reason})")
    
    # æ£€æŸ¥OpenIEä¸­æœ‰ä½†å›¾è°±ä¸­æ²¡æœ‰çš„å®ä½“
    graph_entities = set(attr.get('content', '').lower() for _, attr in G.nodes(data=True) if attr.get('name', '').startswith('entity-'))
    openie_entities = set(entity.lower() for entity in entity_frequencies.keys())
    missing_in_graph = openie_entities - graph_entities
    
    if missing_in_graph:
        print(f"   âš ï¸  OpenIEä¸­å­˜åœ¨ä½†å›¾è°±ä¸­ç¼ºå¤±çš„å®ä½“: {len(missing_in_graph)}")
        # æ‰¾åˆ°åŸå§‹å¤§å°å†™å½¢å¼çš„å®ä½“åç§°
        entity_lowercase_to_original = {entity.lower(): entity for entity in entity_frequencies.keys()}
        for entity_lower in missing_in_graph:
            original_entity = entity_lowercase_to_original.get(entity_lower, entity_lower)
            print(f"     - '{original_entity}' (å¯èƒ½ç”±äºtext_processingå‡½æ•°é—®é¢˜)")
    
    # è¿”å›å›¾å’Œè¯Šæ–­ä¿¡æ¯
    diagnostics = (matched_entities, missing_entities, missing_in_graph)
    return G, diagnostics

def create_fusion_description(node_id, node_data, degrees):
    """åˆ›å»ºèåˆçš„èŠ‚ç‚¹æè¿°"""
    desc_parts = []
    
    if node_id.startswith('entity-'):
        content = node_data.get('content', 'Unknown')
        semantic_name = node_data.get('semantic_name', content)
        contexts = node_data.get('contexts', [])
        relations = node_data.get('relations', [])
        inferred_types = node_data.get('inferred_types', [])
        frequency = node_data.get('frequency', 0)
        
        desc_parts.append(f"ğŸ¯ å®ä½“: {semantic_name}")
        if inferred_types:
            desc_parts.append(f"ğŸ·ï¸ ç±»å‹: {', '.join(inferred_types)}")
        desc_parts.append(f"ğŸ”— å›¾è°±åº¦: {degrees[node_id]}")
        desc_parts.append(f"ğŸ“Š è¯­ä¹‰é¢‘ç‡: {frequency}")
        desc_parts.append(f"ğŸ“‹ å›¾è°±ID: {node_id}")
        desc_parts.append(f"ğŸ”‘ Hash: {node_data.get('hash_id', 'N/A')[:12]}...")
        
        if contexts:
            desc_parts.append(f"ğŸ“– ä¸Šä¸‹æ–‡:")
            for i, context in enumerate(set(contexts[:3]), 1):  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                desc_parts.append(f"  {i}. {context}")
        
        if relations:
            desc_parts.append(f"ğŸ”— è¯­ä¹‰å…³ç³»:")
            unique_relations = list(set(relations[:5]))  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            for i, relation in enumerate(unique_relations, 1):
                desc_parts.append(f"  {i}. {relation}")
    
    elif node_id.startswith('chunk-'):
        content = node_data.get('content', 'Unknown')
        desc_parts.append(f"ğŸ“„ æ–‡æ¡£å—: {content}")
        desc_parts.append(f"ğŸ”— å›¾è°±åº¦: {degrees[node_id]}")
        desc_parts.append(f"ğŸ“‹ å›¾è°±ID: {node_id}")
        desc_parts.append(f"ğŸ”‘ Hash: {node_data.get('hash_id', 'N/A')[:12]}...")
    
    return "\n".join(desc_parts)

def create_fusion_edge_description(source, target, edge_data):
    """åˆ›å»ºèåˆçš„è¾¹æè¿°"""
    weight = edge_data.get('weight', 1.0)
    semantic_relations = edge_data.get('semantic_relations', [])
    
    desc_parts = []
    desc_parts.append(f"ğŸ”— è¿æ¥: {source} â†” {target}")
    desc_parts.append(f"âš–ï¸ åµŒå…¥æƒé‡: {weight:.3f}")
    
    if semantic_relations:
        desc_parts.append(f"ğŸ“ è¯­ä¹‰å…³ç³»:")
        for i, (relation, context) in enumerate(semantic_relations[:3], 1):  # æœ€å¤šæ˜¾ç¤º3ä¸ª
            desc_parts.append(f"  {i}. {relation}")
            desc_parts.append(f"     æ¥æº: {context[:50]}...")
    else:
        desc_parts.append(f"ğŸ“ è¯­ä¹‰å…³ç³»: åŸºäºåµŒå…¥ç›¸ä¼¼åº¦è¿æ¥")
    
    return "\n".join(desc_parts)

def get_entity_color(entity_content, inferred_types):
    """æ ¹æ®å®ä½“å†…å®¹å’Œç±»å‹ç¡®å®šé¢œè‰²"""
    content_lower = entity_content.lower()
    
    if 'politician' in content_lower or 'Profession/Role' in inferred_types:
        return "#96CEB4"  # ç»¿è‰² - èŒä¸š
    elif any(name in content_lower for name in ['cinderella', 'prince', 'oliver badman', 'george rankin', 'thomas marwick', 'erik hort', 'marina']):
        return "#FF6B6B"  # çº¢è‰² - äººç‰©
    elif any(place in content_lower for place in ['montebello', 'minsk', 'rockland county', 'kingdom']) or any(t in inferred_types for t in ['Place', 'Administrative Region']):
        return "#4ECDC4"  # é’è‰² - åœ°ç‚¹
    elif 'slipper' in content_lower or 'Object' in inferred_types:
        return "#45B7D1"  # è“è‰² - ç‰©å“
    elif 'royal ball' in content_lower or any(t in inferred_types for t in ['Place/Event', 'Event']):
        return "#DDA0DD"  # ç´«è‰² - äº‹ä»¶
    else:
        return "#FFEAA7"  # é»„è‰² - å…¶ä»–

def visualize_fusion_graph(G, output_file, diagnostics=None):
    """å¯è§†åŒ–èåˆå›¾è°±"""
    
    # è®¡ç®—åº¦åˆ†å¸ƒ
    degrees = dict(G.degree())
    
    # åˆ†æèŠ‚ç‚¹ç±»å‹
    entity_nodes = [n for n in G.nodes() if n.startswith('entity-')]
    chunk_nodes = [n for n in G.nodes() if n.startswith('chunk-')]
    
    # åˆ›å»ºpyvisç½‘ç»œ
    net = Network(height="900px", width="100%", bgcolor="#f8f9fa", font_color="black")
    
    # è®¾ç½®ç‰©ç†æ•ˆæœ
    net.set_options("""
    var options = {
      "physics": {
        "enabled": true,
        "stabilization": {"iterations": 300},
        "barnesHut": {
          "gravitationalConstant": -15000,
          "centralGravity": 0.15,
          "springLength": 120,
          "springConstant": 0.03,
          "damping": 0.09
        }
      },
      "interaction": {
        "hover": true,
        "selectConnectedEdges": true,
        "tooltipDelay": 300
      },
      "manipulation": {
        "enabled": false
      }
    }
    """)
    
    # æ·»åŠ èŠ‚ç‚¹
    for node_id, node_data in G.nodes(data=True):
        # è®¡ç®—èŠ‚ç‚¹å¤§å°
        node_degree = degrees[node_id]
        frequency = node_data.get('frequency', 0)
        base_size = 18
        size = base_size + node_degree * 3 + frequency * 2
        
        # è®¾ç½®èŠ‚ç‚¹é¢œè‰²å’Œæ ‡ç­¾
        if node_id.startswith('entity-'):
            content = node_data.get('semantic_name', node_data.get('content', ''))
            inferred_types = node_data.get('inferred_types', [])
            
            # ç‰¹æ®Šå¤„ç†ç¼ºå¤±å®ä½“
            if content == "æœªçŸ¥å®ä½“" or content == "":
                color = "#FF0000"  # çº¢è‰² - ç¼ºå¤±å®ä½“
                label = "âŒæœªçŸ¥"
            else:
                color = get_entity_color(content, inferred_types)
                label = content
        elif node_id.startswith('chunk-'):
            color = "#000000"  # é»‘è‰² - æ–‡æ¡£
            label = f"Doc-{node_id[-8:]}"
        else:
            color = "#B2B2B2"  # ç°è‰² - æœªçŸ¥
            label = node_id
        
        # åˆ›å»ºèåˆæè¿°
        description = create_fusion_description(node_id, node_data, degrees)
        
        net.add_node(node_id,
                     label=label,
                     size=size,
                     color=color,
                     title=description,
                     font={'size': 14, 'color': 'black'})
    
    # æ·»åŠ è¾¹
    for source, target, edge_data in G.edges(data=True):
        weight = edge_data.get('weight', 1.0)
        semantic_relations = edge_data.get('semantic_relations', [])
        
        # è¾¹å®½åº¦å’Œé¢œè‰²
        width = max(2, int(weight * 4))
        
        # æ ¹æ®æƒé‡å’Œè¯­ä¹‰å…³ç³»ç¡®å®šé¢œè‰²
        if semantic_relations:
            if weight >= 1.5:
                color = "#E74C3C"  # çº¢è‰² - å¼ºè¯­ä¹‰è¿æ¥
            else:
                color = "#8E44AD"  # ç´«è‰² - å¼±è¯­ä¹‰è¿æ¥
        else:
            if weight >= 1.5:
                color = "#3498DB"  # è“è‰² - å¼ºåµŒå…¥è¿æ¥
            else:
                color = "#95A5A6"  # ç°è‰² - å¼±åµŒå…¥è¿æ¥
        
        # åˆ›å»ºè¾¹æè¿°
        edge_description = create_fusion_edge_description(source, target, edge_data)
        
        net.add_edge(source, target,
                     color=color,
                     width=width,
                     title=edge_description)
    
    # ç”ŸæˆHTML
    html_content = net.generate_html()
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    weight_stats = [G[u][v].get('weight', 1.0) for u, v in G.edges()]
    semantic_edges = sum(1 for u, v in G.edges() if G[u][v].get('semantic_relations'))
    
    # è¯Šæ–­ä¿¡æ¯
    diagnostics_html = ""
    if diagnostics:
        matched_entities, missing_entities, missing_in_graph = diagnostics
        diagnostics_html = f"""
        <h4>ğŸ” å®ä½“åŒ¹é…è¯Šæ–­</h4>
        <p><strong>âœ… æˆåŠŸåŒ¹é…:</strong> {len(matched_entities)}</p>
        <p><strong>âŒ ç¼ºå¤±å®ä½“:</strong> {len(missing_entities)}</p>
        <p><strong>âš ï¸ OpenIEç¼ºå¤±:</strong> {len(missing_in_graph)}</p>
        
        {f'''<div style="margin-top: 10px; padding: 10px; background: #ffebee; border-radius: 5px;">
        <strong>âš ï¸ æ£€æµ‹åˆ°é—®é¢˜:</strong><br>
        OpenIEä¸­æœ‰{len(missing_in_graph)}ä¸ªå®ä½“åœ¨å›¾è°±ä¸­ç¼ºå¤±ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºtext_processingå‡½æ•°çš„é—®é¢˜å¯¼è‡´çš„ã€‚<br>
        <strong>ç¼ºå¤±å®ä½“:</strong> {", ".join(list(missing_in_graph)[:5])}{"..." if len(missing_in_graph) > 5 else ""}
        </div>''' if missing_in_graph else ""}
        """
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯é¢æ¿
    stats_panel = f"""
    <div style="position: fixed; top: 10px; right: 10px; width: 380px; background: white; 
                border: 1px solid #ccc; padding: 15px; border-radius: 8px; font-family: Arial; z-index: 1000; max-height: 80vh; overflow-y: auto;">
        <h3>ğŸ­ èåˆçŸ¥è¯†å›¾è°±</h3>
        <p><strong>èŠ‚ç‚¹æ•°é‡:</strong> {G.number_of_nodes()}</p>
        <p><strong>è¾¹æ•°é‡:</strong> {G.number_of_edges()}</p>
        <p><strong>å®ä½“èŠ‚ç‚¹:</strong> {len(entity_nodes)}</p>
        <p><strong>æ–‡æ¡£å—èŠ‚ç‚¹:</strong> {len(chunk_nodes)}</p>
        <p><strong>è¯­ä¹‰è¾¹:</strong> {semantic_edges}</p>
        <p><strong>åµŒå…¥è¾¹:</strong> {G.number_of_edges() - semantic_edges}</p>
        <p><strong>æƒé‡èŒƒå›´:</strong> {min(weight_stats):.3f} - {max(weight_stats):.3f}</p>
        
        {diagnostics_html}
        
        <h4>ğŸ”¥ åº¦æœ€é«˜çš„èŠ‚ç‚¹</h4>
        <ul>
        {''.join([f"<li>{node[-8:]}: {deg}</li>" for node, deg in sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:3]])}
        </ul>
        
        <h4>ğŸ¨ èŠ‚ç‚¹é¢œè‰²è¯´æ˜</h4>
        <ul>
            <li>ğŸ”´ çº¢è‰²: äººç‰©å®ä½“/ç¼ºå¤±å®ä½“</li>
            <li>ğŸŸ¢ é’è‰²: åœ°ç‚¹å®ä½“</li>
            <li>ğŸ”µ è“è‰²: ç‰©å“å®ä½“</li>
            <li>ğŸŸ¢ ç»¿è‰²: èŒä¸šå®ä½“</li>
            <li>ğŸŸ£ ç´«è‰²: äº‹ä»¶å®ä½“</li>
            <li>ğŸŸ¡ é»„è‰²: å…¶ä»–å®ä½“</li>
            <li>âš« é»‘è‰²: æ–‡æ¡£å—</li>
        </ul>
        
        <h4>ğŸ”— è¾¹é¢œè‰²è¯´æ˜</h4>
        <ul>
            <li>ğŸ”´ çº¢è‰²: å¼ºè¯­ä¹‰è¿æ¥</li>
            <li>ğŸŸ£ ç´«è‰²: å¼±è¯­ä¹‰è¿æ¥</li>
            <li>ğŸ”µ è“è‰²: å¼ºåµŒå…¥è¿æ¥</li>
            <li>âš« ç°è‰²: å¼±åµŒå…¥è¿æ¥</li>
        </ul>
        
        <h4>ğŸ’¡ ç‰¹è‰²åŠŸèƒ½</h4>
        <ul>
            <li>âœ¨ èåˆå›¾è°±ç»“æ„å’Œè¯­ä¹‰</li>
            <li>ğŸ¯ æ˜¾ç¤ºå®ä½“ç±»å‹å’Œä¸Šä¸‹æ–‡</li>
            <li>ğŸ“Š å±•ç¤ºåµŒå…¥æƒé‡å’Œè¯­ä¹‰å…³ç³»</li>
            <li>ğŸ” æ‚¬åœæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯</li>
            <li>âš ï¸ è‡ªåŠ¨æ£€æµ‹å®ä½“åŒ¹é…é—®é¢˜</li>
        </ul>
    </div>
    """
    
    # æ’å…¥ç»Ÿè®¡é¢æ¿
    html_content = html_content.replace('<body>', f'<body>{stats_panel}')
    
    # ä¿å­˜æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ… èåˆå›¾è°±å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_file}")
    
    return G

def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    pickle_file = "outputs/deepseek-v3_bge-m3/graph.pickle"
    openie_file = "outputs/openie_results_ner_deepseek-v3.json"
    
    print("ğŸ“š åŠ è½½HippoRAGå›¾è°±å’ŒOpenIEæ•°æ®...")
    
    # åŠ è½½æ•°æ®
    igraph_obj = load_hipporag_graph(pickle_file)
    openie_data = load_openie_data(openie_file)
    
    if igraph_obj is None or openie_data is None:
        return
    
    print("ğŸ” æå–OpenIEè¯­ä¹‰ä¿¡æ¯...")
    openie_semantics = extract_openie_semantics(openie_data)
    
    print("ğŸ”„ è½¬æ¢ä¸ºèåˆNetworkXå›¾...")
    G, diagnostics = igraph_to_networkx_with_semantics(igraph_obj, openie_semantics)
    
    print("ğŸ¨ ç”Ÿæˆèåˆå¯è§†åŒ–...")
    output_file = "fusion_knowledge_graph.html"
    visualize_fusion_graph(G, output_file, diagnostics)
    
    print(f"\nğŸ‰ å®Œæˆï¼è¯·æ‰“å¼€ {output_file} æŸ¥çœ‹èåˆçŸ¥è¯†å›¾è°±")
    print("âœ¨ èåˆå›¾è°±ç‰¹è‰²:")
    print("   - ğŸ§  ä¿æŒHippoRAGçš„çœŸå®å›¾è°±ç»“æ„")
    print("   - ğŸ“ èåˆOpenIEçš„è¯­ä¹‰å…³ç³»æè¿°")
    print("   - ğŸ¯ å±•ç¤ºå®ä½“ç±»å‹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯")
    print("   - âš–ï¸ æ˜¾ç¤ºåµŒå…¥æƒé‡å’Œè¯­ä¹‰å…³ç³»")
    print("   - ğŸ” æä¾›æœ€ä¸°å¯Œçš„äº¤äº’å¼è¯¦æƒ…")

if __name__ == "__main__":
    main() 