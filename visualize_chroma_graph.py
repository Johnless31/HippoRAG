#!/usr/bin/env python3
"""
åŸºäºChromaDBçš„HippoRAGå›¾è°±å¯è§†åŒ–
ä»ChromaDBå­˜å‚¨ä¸­åŠ è½½å›¾è°±æ•°æ®å¹¶ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–
"""

import os
import sys
import json
import pickle
import networkx as nx
from pyvis.network import Network
from collections import defaultdict, Counter
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('src')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    import igraph as ig
except ImportError:
    logger.error("âŒ éœ€è¦å®‰è£…igraph: pip install igraph")
    sys.exit(1)

try:
    import chromadb
    from src.hipporag.chroma_store import ChromaStore
    CHROMA_AVAILABLE = True
except ImportError:
    logger.warning("âš ï¸ ChromaDBä¸å¯ç”¨ï¼Œå°†ä»…æ”¯æŒä¼ ç»Ÿembedding store")
    ChromaStore = None
    CHROMA_AVAILABLE = False

class EmbeddingStoreAdapter:
    """é€‚é…å™¨ç±»ï¼Œç”¨äºå…¼å®¹ä¼ ç»Ÿembedding storeæ•°æ®æ ¼å¼"""
    
    def __init__(self, data: Dict):
        self.data = data
    
    def count(self) -> int:
        return len(self.data)
    
    def get_all_ids(self) -> List[str]:
        return list(self.data.keys())
    
    def get_all_texts(self) -> List[str]:
        return [item['content'] for item in self.data.values()]
    
    def get_all_id_to_rows(self) -> Dict[str, Dict[str, str]]:
        return self.data
    
    def get_row(self, hash_id: str) -> Dict[str, str]:
        if hash_id not in self.data:
            raise KeyError(f"Hash ID {hash_id} not found")
        return self.data[hash_id]
    
    def get_rows(self, hash_ids: List[str]) -> Dict[str, Dict[str, str]]:
        return {hash_id: self.data[hash_id] for hash_id in hash_ids if hash_id in self.data}

class ChromaGraphVisualizer:
    """åŸºäºChromaDBçš„HippoRAGå›¾è°±å¯è§†åŒ–å™¨"""
    
    def __init__(self, working_dir: str, namespace: str = "default"):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            working_dir: HippoRAGå·¥ä½œç›®å½•
            namespace: å‘½åç©ºé—´ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å®éªŒ
        """
        self.working_dir = working_dir
        self.namespace = namespace
        
        # ChromaDBè·¯å¾„
        self.chunk_db_path = os.path.join(working_dir, "chroma_chunk_embeddings")
        self.entity_db_path = os.path.join(working_dir, "chroma_entity_embeddings")
        self.fact_db_path = os.path.join(working_dir, "chroma_fact_embeddings")
        
        # å›¾è°±æ–‡ä»¶è·¯å¾„
        self.graph_pickle_path = os.path.join(working_dir, "graph.pickle")
        self.openie_results_path = None
        
        # åˆå§‹åŒ–ChromaStore
        self.chunk_store = None
        self.entity_store = None
        self.fact_store = None
        
        # æ•°æ®å®¹å™¨
        self.igraph_obj = None
        self.openie_data = None
        
        logger.info(f"ChromaGraphVisualizer initialized for {working_dir}")
    
    def load_chroma_stores(self):
        """åŠ è½½ChromaDBå­˜å‚¨"""
        logger.info("ğŸ”„ åŠ è½½ChromaDBå­˜å‚¨...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ChromaDBæ•°æ®
        chroma_exists = (
            os.path.exists(self.chunk_db_path) and 
            os.path.exists(self.entity_db_path) and 
            os.path.exists(self.fact_db_path)
        )
        
        if not chroma_exists or not CHROMA_AVAILABLE:
            logger.info("âš ï¸ æœªæ‰¾åˆ°ChromaDBæ•°æ®æˆ–ChromaDBä¸å¯ç”¨ï¼Œå°è¯•ä»ä¼ ç»Ÿembedding storeè½¬æ¢...")
            return self._convert_from_embedding_store()
        
        try:
            # åˆå§‹åŒ–ChromaStoreï¼ˆä¸éœ€è¦embeddingæ¨¡å‹ï¼Œåªç”¨äºæ•°æ®è®¿é—®ï¼‰
            self.chunk_store = ChromaStore(None, self.chunk_db_path, 32, 'chunk')
            self.entity_store = ChromaStore(None, self.entity_db_path, 32, 'entity')
            self.fact_store = ChromaStore(None, self.fact_db_path, 32, 'fact')
            
            # æ£€æŸ¥æ•°æ®
            chunk_count = self.chunk_store.count()
            entity_count = self.entity_store.count()
            fact_count = self.fact_store.count()
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ChromaDBå­˜å‚¨:")
            logger.info(f"   - æ–‡æ¡£å—: {chunk_count}")
            logger.info(f"   - å®ä½“: {entity_count}")
            logger.info(f"   - äº‹å®: {fact_count}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ChromaDBå­˜å‚¨å¤±è´¥: {e}")
            return False
    
    def _convert_from_embedding_store(self):
        """ä»ä¼ ç»Ÿembedding storeè½¬æ¢æ•°æ®"""
        logger.info("ğŸ”„ ä»ä¼ ç»Ÿembedding storeåŠ è½½æ•°æ®...")
        
        # æ£€æŸ¥ä¼ ç»Ÿembedding storeè·¯å¾„
        traditional_chunk_path = os.path.join(self.working_dir, "chunk_embeddings")
        traditional_entity_path = os.path.join(self.working_dir, "entity_embeddings")
        traditional_fact_path = os.path.join(self.working_dir, "fact_embeddings")
        
        if not (os.path.exists(traditional_chunk_path) and 
                os.path.exists(traditional_entity_path) and 
                os.path.exists(traditional_fact_path)):
            logger.error("âŒ æœªæ‰¾åˆ°ä¼ ç»Ÿembedding storeæ•°æ®")
            return False
        
        try:
            # ä»ä¼ ç»Ÿembedding storeåŠ è½½æ•°æ®
            self.chunk_data = self._load_traditional_embedding_store(traditional_chunk_path, 'chunk')
            self.entity_data = self._load_traditional_embedding_store(traditional_entity_path, 'entity')
            self.fact_data = self._load_traditional_embedding_store(traditional_fact_path, 'fact')
            
            logger.info(f"âœ… æˆåŠŸä»ä¼ ç»Ÿembedding storeåŠ è½½æ•°æ®:")
            logger.info(f"   - æ–‡æ¡£å—: {len(self.chunk_data)}")
            logger.info(f"   - å®ä½“: {len(self.entity_data)}")
            logger.info(f"   - äº‹å®: {len(self.fact_data)}")
            
            # åˆ›å»ºæ¨¡æ‹Ÿçš„ChromaStoreå¯¹è±¡
            self.chunk_store = EmbeddingStoreAdapter(self.chunk_data)
            self.entity_store = EmbeddingStoreAdapter(self.entity_data)
            self.fact_store = EmbeddingStoreAdapter(self.fact_data)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä»ä¼ ç»Ÿembedding storeåŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _load_traditional_embedding_store(self, db_path: str, namespace: str) -> Dict:
        """åŠ è½½ä¼ ç»Ÿembedding storeæ•°æ®"""
        parquet_file = os.path.join(db_path, f"vdb_{namespace}.parquet")
        if not os.path.exists(parquet_file):
            logger.error(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {parquet_file}")
            return {}
        
        df = pd.read_parquet(parquet_file)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        data = {}
        for _, row in df.iterrows():
            data[row['hash_id']] = {
                'hash_id': row['hash_id'],
                'content': row['content']
            }
        
        return data
    
    def load_graph(self):
        """åŠ è½½å›¾è°±ç»“æ„"""
        logger.info("ğŸ”„ åŠ è½½å›¾è°±ç»“æ„...")
        
        if not os.path.exists(self.graph_pickle_path):
            logger.error(f"âŒ æ‰¾ä¸åˆ°å›¾è°±æ–‡ä»¶: {self.graph_pickle_path}")
            return False
        
        try:
            with open(self.graph_pickle_path, 'rb') as f:
                self.igraph_obj = pickle.load(f)
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½å›¾è°±: {self.igraph_obj.summary()}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å›¾è°±å¤±è´¥: {e}")
            return False
    
    def load_openie_data(self):
        """åŠ è½½OpenIEæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
        # æŸ¥æ‰¾OpenIEç»“æœæ–‡ä»¶
        possible_files = [
            os.path.join(self.working_dir, "openie_results.json"),
            os.path.join(os.path.dirname(self.working_dir), "openie_results_ner_deepseek-v3.json"),
            os.path.join(os.path.dirname(self.working_dir), "openie_results.json"),
        ]
        
        for file_path in possible_files:
            if os.path.exists(file_path):
                self.openie_results_path = file_path
                break
        
        if self.openie_results_path is None:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°OpenIEç»“æœæ–‡ä»¶ï¼Œå°†ä½¿ç”¨åŸºæœ¬çš„å›¾è°±ä¿¡æ¯")
            return False
        
        try:
            with open(self.openie_results_path, 'r', encoding='utf-8') as f:
                self.openie_data = json.load(f)
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½OpenIEæ•°æ®: {len(self.openie_data.get('docs', []))} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½OpenIEæ•°æ®å¤±è´¥: {e}")
            return False
    
    def extract_semantic_info(self):
        """ä»OpenIEæ•°æ®ä¸­æå–è¯­ä¹‰ä¿¡æ¯"""
        if not self.openie_data:
            return {}, {}, {}, Counter(), {}
        
        entity_contexts = defaultdict(list)
        entity_relations = defaultdict(list)
        entity_types = defaultdict(set)
        relation_mappings = defaultdict(list)
        entity_frequencies = Counter()
        
        for doc in self.openie_data.get('docs', []):
            passage = doc.get('passage', '')
            entities = doc.get('extracted_entities', [])
            triples = doc.get('extracted_triples', [])
            
            # æ”¶é›†æ‰€æœ‰å®ä½“
            all_entities = set(entities)
            for triple in triples:
                if len(triple) >= 3:
                    subject, relation, obj = triple[0], triple[1], triple[2]
                    all_entities.add(subject)
                    all_entities.add(obj)
            
            # è®°å½•ä¸Šä¸‹æ–‡å’Œé¢‘ç‡
            for entity in all_entities:
                entity_contexts[entity].append(passage)
                entity_frequencies[entity] += 1
            
            # è®°å½•å…³ç³»
            for triple in triples:
                if len(triple) >= 3:
                    subject, relation, obj = triple[0], triple[1], triple[2]
                    relation_str = f"{subject} â†’ {relation} â†’ {obj}"
                    entity_relations[subject].append(relation_str)
                    entity_relations[obj].append(relation_str)
                    relation_mappings[(subject, obj)].append((relation, passage))
                    relation_mappings[(obj, subject)].append((relation, passage))
        
        # æ¨æ–­å®ä½“ç±»å‹
        for entity in entity_frequencies:
            relations = entity_relations[entity]
            inferred_types = self._infer_entity_type(entity, relations)
            entity_types[entity].update(inferred_types)
        
        return entity_contexts, entity_relations, entity_types, entity_frequencies, relation_mappings
    
    def _infer_entity_type(self, entity_name: str, relations: List[str]) -> List[str]:
        """ä»å…³ç³»ä¸­æ¨æ–­å®ä½“ç±»å‹"""
        entity_types = set()
        
        for relation_str in relations:
            if "is a" in relation_str:
                parts = relation_str.split(" â†’ ")
                if len(parts) == 3 and parts[1] == "is a" and parts[0] == entity_name:
                    entity_types.add(parts[2])
            elif "born in" in relation_str or "birthplace" in relation_str:
                if entity_name in relation_str.split(" â†’ ")[0]:
                    entity_types.add("Person")
                else:
                    entity_types.add("Place")
        
        # æ ¹æ®å®ä½“åç§°æ¨æ–­
        if "County" in entity_name:
            entity_types.add("Administrative Region")
        elif entity_name.lower() in ["cinderella", "prince"]:
            entity_types.add("Character")
        elif entity_name.lower() in ["kingdom", "royal ball"]:
            entity_types.add("Place/Event")
        elif "slipper" in entity_name.lower():
            entity_types.add("Object")
        elif entity_name.lower() == "politician":
            entity_types.add("Profession/Role")
        
        return list(entity_types)
    
    def build_networkx_graph(self):
        """æ„å»ºNetworkXå›¾"""
        logger.info("ğŸ”„ æ„å»ºNetworkXå›¾...")
        
        if not self.igraph_obj:
            logger.error("âŒ å›¾è°±æœªåŠ è½½")
            return None
        
        # æå–è¯­ä¹‰ä¿¡æ¯
        entity_contexts, entity_relations, entity_types, entity_frequencies, relation_mappings = self.extract_semantic_info()
        
        # åˆ›å»ºNetworkXå›¾
        G = nx.Graph()
        
        # è·å–ChromaDBæ•°æ®
        chunk_data = self.chunk_store.get_all_id_to_rows()
        entity_data = self.entity_store.get_all_id_to_rows()
        fact_data = self.fact_store.get_all_id_to_rows()
        
        # è¯Šæ–­ä¿¡æ¯
        matched_entities = []
        missing_entities = []
        
        # æ·»åŠ èŠ‚ç‚¹
        for i in range(self.igraph_obj.vcount()):
            node_attrs = {}
            for attr in self.igraph_obj.vertex_attributes():
                node_attrs[attr] = self.igraph_obj.vs[i][attr]
            
            node_id = node_attrs.get('name', f'node_{i}')
            node_content = node_attrs.get('content', '').lower()
            
            # èåˆè¯­ä¹‰ä¿¡æ¯
            if node_id.startswith('entity-'):
                # æŸ¥æ‰¾å¯¹åº”çš„å®ä½“
                matching_entity = None
                for entity in entity_frequencies:
                    if entity.lower() == node_content:
                        matching_entity = entity
                        break
                
                if matching_entity:
                    matched_entities.append((node_id, matching_entity, node_content))
                    node_attrs['semantic_name'] = matching_entity
                    node_attrs['contexts'] = entity_contexts[matching_entity]
                    node_attrs['relations'] = entity_relations[matching_entity]
                    node_attrs['inferred_types'] = list(entity_types[matching_entity])
                    node_attrs['frequency'] = entity_frequencies[matching_entity]
                else:
                    missing_entities.append((node_id, node_content))
                    node_attrs['semantic_name'] = node_content if node_content else "æœªçŸ¥å®ä½“"
                    node_attrs['contexts'] = []
                    node_attrs['relations'] = []
                    node_attrs['inferred_types'] = []
                    node_attrs['frequency'] = 0
            
            elif node_id.startswith('chunk-'):
                # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡£å—
                if node_id in chunk_data:
                    chunk_info = chunk_data[node_id]
                    node_attrs['full_content'] = chunk_info['content']
                
            G.add_node(node_id, **node_attrs)
        
        # æ·»åŠ è¾¹
        for i in range(self.igraph_obj.ecount()):
            edge = self.igraph_obj.es[i]
            source_idx = edge.source
            target_idx = edge.target
            
            source_name = self.igraph_obj.vs[source_idx]['name']
            target_name = self.igraph_obj.vs[target_idx]['name']
            
            edge_attrs = {}
            for attr in self.igraph_obj.edge_attributes():
                edge_attrs[attr] = edge[attr]
            
            # èåˆè¯­ä¹‰å…³ç³»ä¿¡æ¯
            source_content = self.igraph_obj.vs[source_idx]['content'] if 'content' in self.igraph_obj.vs[source_idx].attributes() else ''
            target_content = self.igraph_obj.vs[target_idx]['content'] if 'content' in self.igraph_obj.vs[target_idx].attributes() else ''
            
            # æŸ¥æ‰¾è¯­ä¹‰å…³ç³»
            semantic_relations = []
            if (source_content, target_content) in relation_mappings:
                semantic_relations.extend(relation_mappings[(source_content, target_content)])
            if (target_content, source_content) in relation_mappings:
                semantic_relations.extend(relation_mappings[(target_content, source_content)])
            
            edge_attrs['semantic_relations'] = semantic_relations
            
            G.add_edge(source_name, target_name, **edge_attrs)
        
        # æ‰“å°è¯Šæ–­ä¿¡æ¯
        logger.info(f"ğŸ” å›¾è°±æ„å»ºå®Œæˆ:")
        logger.info(f"   - èŠ‚ç‚¹æ€»æ•°: {G.number_of_nodes()}")
        logger.info(f"   - è¾¹æ€»æ•°: {G.number_of_edges()}")
        logger.info(f"   - æˆåŠŸåŒ¹é…å®ä½“: {len(matched_entities)}")
        logger.info(f"   - ç¼ºå¤±å®ä½“: {len(missing_entities)}")
        
        # è¿”å›å›¾å’Œè¯Šæ–­ä¿¡æ¯
        diagnostics = (matched_entities, missing_entities, [])
        return G, diagnostics
    
    def get_entity_color(self, entity_content: str, inferred_types: List[str]) -> str:
        """æ ¹æ®å®ä½“å†…å®¹å’Œç±»å‹ç¡®å®šé¢œè‰²"""
        content_lower = entity_content.lower()
        
        if 'politician' in content_lower or 'Profession/Role' in inferred_types:
            return "#96CEB4"  # ç»¿è‰² - èŒä¸š
        elif any(name in content_lower for name in ['cinderella', 'prince', 'person']):
            return "#FF6B6B"  # çº¢è‰² - äººç‰©
        elif any(place in content_lower for place in ['county', 'kingdom', 'place']) or any(t in inferred_types for t in ['Place', 'Administrative Region']):
            return "#4ECDC4"  # é’è‰² - åœ°ç‚¹
        elif 'slipper' in content_lower or 'Object' in inferred_types:
            return "#45B7D1"  # è“è‰² - ç‰©å“
        elif 'event' in content_lower or any(t in inferred_types for t in ['Place/Event', 'Event']):
            return "#DDA0DD"  # ç´«è‰² - äº‹ä»¶
        else:
            return "#FFEAA7"  # é»„è‰² - å…¶ä»–
    
    def create_node_description(self, node_id: str, node_data: Dict, degrees: Dict) -> str:
        """åˆ›å»ºèŠ‚ç‚¹æè¿°"""
        desc_parts = []
        
        if node_id.startswith('entity-'):
            content = node_data.get('content', 'Unknown')
            semantic_name = node_data.get('semantic_name', content)
            contexts = node_data.get('contexts', [])
            relations = node_data.get('relations', [])
            inferred_types = node_data.get('inferred_types', [])
            frequency = node_data.get('frequency', 0)
            
            desc_parts.append(f"ğŸ¯ å®ä½“: {semantic_name}")
            if inferred_types:
                desc_parts.append(f"ğŸ·ï¸ ç±»å‹: {', '.join(inferred_types)}")
            desc_parts.append(f"ğŸ”— å›¾è°±åº¦: {degrees[node_id]}")
            desc_parts.append(f"ğŸ“Š è¯­ä¹‰é¢‘ç‡: {frequency}")
            desc_parts.append(f"ğŸ“‹ èŠ‚ç‚¹ID: {node_id}")
            desc_parts.append(f"ğŸ”‘ Hash: {node_data.get('hash_id', 'N/A')}")
            
            if contexts:
                desc_parts.append(f"ğŸ“– ä¸Šä¸‹æ–‡:")
                for i, context in enumerate(set(contexts[:3]), 1):
                    desc_parts.append(f"  {i}. {context[:100]}...")
            
            if relations:
                desc_parts.append(f"ğŸ”— è¯­ä¹‰å…³ç³»:")
                unique_relations = list(set(relations[:5]))
                for i, relation in enumerate(unique_relations, 1):
                    desc_parts.append(f"  {i}. {relation}")
        
        elif node_id.startswith('chunk-'):
            content = node_data.get('content', 'Unknown')
            full_content = node_data.get('full_content', content)
            desc_parts.append(f"ğŸ“„ æ–‡æ¡£å—: {full_content[:100]}...")
            desc_parts.append(f"ğŸ”— å›¾è°±åº¦: {degrees[node_id]}")
            desc_parts.append(f"ğŸ“‹ èŠ‚ç‚¹ID: {node_id}")
            desc_parts.append(f"ğŸ”‘ Hash: {node_data.get('hash_id', 'N/A')}")
        
        return "\n".join(desc_parts)
    
    def create_edge_description(self, source: str, target: str, edge_data: Dict) -> str:
        """åˆ›å»ºè¾¹æè¿°"""
        weight = edge_data.get('weight', 1.0)
        semantic_relations = edge_data.get('semantic_relations', [])
        
        desc_parts = []
        desc_parts.append(f"ğŸ”— è¿æ¥: {source} â†” {target}")
        desc_parts.append(f"âš–ï¸ æƒé‡: {weight:.3f}")
        
        if semantic_relations:
            desc_parts.append(f"ğŸ“ è¯­ä¹‰å…³ç³»:")
            for i, (relation, context) in enumerate(semantic_relations[:3], 1):
                desc_parts.append(f"  {i}. {relation}")
                desc_parts.append(f"     æ¥æº: {context[:50]}...")
        else:
            desc_parts.append(f"ğŸ“ è¯­ä¹‰å…³ç³»: åŸºäºåµŒå…¥ç›¸ä¼¼åº¦è¿æ¥")
        
        return "\n".join(desc_parts)
    
    def visualize_graph(self, G: nx.Graph, output_file: str, diagnostics: Tuple = None):
        """å¯è§†åŒ–å›¾è°±"""
        logger.info("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–...")
        
        # è®¡ç®—åº¦åˆ†å¸ƒ
        degrees = dict(G.degree())
        
        # åˆ†æèŠ‚ç‚¹ç±»å‹
        entity_nodes = [n for n in G.nodes() if n.startswith('entity-')]
        chunk_nodes = [n for n in G.nodes() if n.startswith('chunk-')]
        
        # åˆ›å»ºpyvisç½‘ç»œ
        net = Network(height="900px", width="100%", bgcolor="#f8f9fa", font_color="black")
        
        # è®¾ç½®ç‰©ç†æ•ˆæœ
        net.set_options("""
        var options = {
          "physics": {
            "enabled": true,
            "stabilization": {"iterations": 300},
            "barnesHut": {
              "gravitationalConstant": -15000,
              "centralGravity": 0.15,
              "springLength": 120,
              "springConstant": 0.03,
              "damping": 0.09
            }
          },
          "interaction": {
            "hover": true,
            "selectConnectedEdges": true,
            "tooltipDelay": 300
          },
          "manipulation": {
            "enabled": false
          }
        }
        """)
        
        # æ·»åŠ èŠ‚ç‚¹
        for node_id, node_data in G.nodes(data=True):
            node_degree = degrees[node_id]
            frequency = node_data.get('frequency', 0)
            base_size = 18
            size = base_size + node_degree * 3 + frequency * 2
            
            if node_id.startswith('entity-'):
                content = node_data.get('semantic_name', node_data.get('content', ''))
                inferred_types = node_data.get('inferred_types', [])
                
                if content == "æœªçŸ¥å®ä½“" or content == "":
                    color = "#FF0000"  # çº¢è‰² - ç¼ºå¤±å®ä½“
                    label = "âŒæœªçŸ¥"
                else:
                    color = self.get_entity_color(content, inferred_types)
                    label = content[:15] + "..." if len(content) > 15 else content
            elif node_id.startswith('chunk-'):
                color = "#000000"  # é»‘è‰² - æ–‡æ¡£
                label = f"Doc-{node_id[-8:]}"
            else:
                color = "#B2B2B2"  # ç°è‰² - æœªçŸ¥
                label = node_id
            
            # åˆ›å»ºæè¿°
            description = self.create_node_description(node_id, node_data, degrees)
            
            net.add_node(node_id,
                         label=label,
                         size=size,
                         color=color,
                         title=description,
                         font={'size': 14, 'color': 'black'})
        
        # æ·»åŠ è¾¹
        for source, target, edge_data in G.edges(data=True):
            weight = edge_data.get('weight', 1.0)
            semantic_relations = edge_data.get('semantic_relations', [])
            
            width = max(2, int(weight * 4))
            
            if semantic_relations:
                if weight >= 1.5:
                    color = "#E74C3C"  # çº¢è‰² - å¼ºè¯­ä¹‰è¿æ¥
                else:
                    color = "#8E44AD"  # ç´«è‰² - å¼±è¯­ä¹‰è¿æ¥
            else:
                if weight >= 1.5:
                    color = "#3498DB"  # è“è‰² - å¼ºåµŒå…¥è¿æ¥
                else:
                    color = "#95A5A6"  # ç°è‰² - å¼±åµŒå…¥è¿æ¥
            
            edge_description = self.create_edge_description(source, target, edge_data)
            
            net.add_edge(source, target,
                         color=color,
                         width=width,
                         title=edge_description)
        
        # ç”ŸæˆHTML
        html_content = net.generate_html()
        
        # ç»Ÿè®¡ä¿¡æ¯
        weight_stats = [G[u][v].get('weight', 1.0) for u, v in G.edges()]
        semantic_edges = sum(1 for u, v in G.edges() if G[u][v].get('semantic_relations'))
        
        # è¯Šæ–­ä¿¡æ¯
        diagnostics_html = ""
        if diagnostics:
            matched_entities, missing_entities, missing_in_graph = diagnostics
            data_source = "ChromaDB" if (ChromaStore and isinstance(self.chunk_store, ChromaStore)) else "ä¼ ç»ŸEmbedding Store"
            diagnostics_html = f"""
            <h4>ğŸ” æ•°æ®æºè¯Šæ–­</h4>
            <p><strong>æ•°æ®æºç±»å‹:</strong> {data_source}</p>
            <p><strong>âœ… æˆåŠŸåŒ¹é…:</strong> {len(matched_entities)}</p>
            <p><strong>âŒ ç¼ºå¤±å®ä½“:</strong> {len(missing_entities)}</p>
            <p><strong>ğŸ“Š æ•°æ®ç»Ÿè®¡:</strong></p>
            <ul>
                <li>æ–‡æ¡£å—: {self.chunk_store.count()}</li>
                <li>å®ä½“: {self.entity_store.count()}</li>
                <li>äº‹å®: {self.fact_store.count()}</li>
            </ul>
            """
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯é¢æ¿
        data_source = "ChromaDB" if (ChromaStore and isinstance(self.chunk_store, ChromaStore)) else "ä¼ ç»ŸEmbedding Store"
        data_source_icon = "ğŸ—„ï¸" if (ChromaStore and isinstance(self.chunk_store, ChromaStore)) else "ğŸ“"
        stats_panel = f"""
        <div style="position: fixed; top: 10px; right: 10px; width: 380px; background: white; 
                    border: 1px solid #ccc; padding: 15px; border-radius: 8px; font-family: Arial; z-index: 1000; max-height: 80vh; overflow-y: auto;">
            <h3>ğŸ­ HippoRAGçŸ¥è¯†å›¾è°±</h3>
            <p><strong>æ•°æ®æº:</strong> {data_source_icon} {data_source}</p>
            <p><strong>å·¥ä½œç›®å½•:</strong> {os.path.basename(self.working_dir)}</p>
            <p><strong>èŠ‚ç‚¹æ•°é‡:</strong> {G.number_of_nodes()}</p>
            <p><strong>è¾¹æ•°é‡:</strong> {G.number_of_edges()}</p>
            <p><strong>å®ä½“èŠ‚ç‚¹:</strong> {len(entity_nodes)}</p>
            <p><strong>æ–‡æ¡£å—èŠ‚ç‚¹:</strong> {len(chunk_nodes)}</p>
            <p><strong>è¯­ä¹‰è¾¹:</strong> {semantic_edges}</p>
            <p><strong>åµŒå…¥è¾¹:</strong> {G.number_of_edges() - semantic_edges}</p>
            {f"<p><strong>æƒé‡èŒƒå›´:</strong> {min(weight_stats):.3f} - {max(weight_stats):.3f}</p>" if weight_stats else ""}
            
            {diagnostics_html}
            
            <h4>ğŸ”¥ åº¦æœ€é«˜çš„èŠ‚ç‚¹</h4>
            <ul>
            {''.join([f"<li>{node[-8:]}: {deg}</li>" for node, deg in sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:3]])}
            </ul>
            
            <h4>ğŸ¨ èŠ‚ç‚¹é¢œè‰²è¯´æ˜</h4>
            <ul>
                <li>ğŸ”´ çº¢è‰²: äººç‰©å®ä½“/ç¼ºå¤±å®ä½“</li>
                <li>ğŸŸ¢ é’è‰²: åœ°ç‚¹å®ä½“</li>
                <li>ğŸ”µ è“è‰²: ç‰©å“å®ä½“</li>
                <li>ğŸŸ¢ ç»¿è‰²: èŒä¸šå®ä½“</li>
                <li>ğŸŸ£ ç´«è‰²: äº‹ä»¶å®ä½“</li>
                <li>ğŸŸ¡ é»„è‰²: å…¶ä»–å®ä½“</li>
                <li>âš« é»‘è‰²: æ–‡æ¡£å—</li>
            </ul>
            
            <h4>ğŸ”— è¾¹é¢œè‰²è¯´æ˜</h4>
            <ul>
                <li>ğŸ”´ çº¢è‰²: å¼ºè¯­ä¹‰è¿æ¥</li>
                <li>ğŸŸ£ ç´«è‰²: å¼±è¯­ä¹‰è¿æ¥</li>
                <li>ğŸ”µ è“è‰²: å¼ºåµŒå…¥è¿æ¥</li>
                <li>âš« ç°è‰²: å¼±åµŒå…¥è¿æ¥</li>
            </ul>
            
            <h4>ğŸ’¡ æ•°æ®æºç‰¹è‰²</h4>
            <ul>
                {'<li>ğŸ—„ï¸ åŸºäºChromaDBå‘é‡å­˜å‚¨</li><li>ğŸš€ é«˜æ•ˆçš„å‘é‡æ£€ç´¢</li><li>ğŸ“Š æŒä¹…åŒ–å­˜å‚¨</li><li>ğŸ” è¯­ä¹‰æœç´¢èƒ½åŠ›</li><li>âš¡ é¿å…å†…å­˜çˆ†ç‚¸</li>' if (ChromaStore and isinstance(self.chunk_store, ChromaStore)) else '<li>ğŸ“ åŸºäºä¼ ç»ŸEmbedding Store</li><li>ğŸ”„ è‡ªåŠ¨é€‚é…å…¼å®¹æ¨¡å¼</li><li>ğŸ“‹ Parquetæ ¼å¼å­˜å‚¨</li><li>âš¡ å¿«é€Ÿæ–‡ä»¶åŠ è½½</li><li>ğŸ”§ å‘åå…¼å®¹æ€§</li>'}
            </ul>
        </div>
        """
        
        # æ’å…¥ç»Ÿè®¡é¢æ¿
        html_content = html_content.replace('<body>', f'<body>{stats_panel}')
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"âœ… ChromaDBå›¾è°±å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_file}")
        
        return G

def main():
    """ä¸»å‡½æ•°"""
    # ç›´æ¥ä½¿ç”¨ test_outputs/deepseek-v3_bge-m3 ç›®å½•
    working_dir = "./outputs/tenant_1/deepseek-v3_bge-m3"
    
    if not os.path.exists(working_dir):
        print(f"âŒ å·¥ä½œç›®å½•ä¸å­˜åœ¨: {working_dir}")
        return
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ChromaDBæ•°æ®
    chroma_chunk_path = os.path.join(working_dir, "chroma_chunk_embeddings")
    traditional_chunk_path = os.path.join(working_dir, "chunk_embeddings")
    
    if not (os.path.exists(chroma_chunk_path) or os.path.exists(traditional_chunk_path)):
        print(f"âŒ ç›®å½•ä¸­æœªæ‰¾åˆ°ChromaDBæˆ–ä¼ ç»Ÿembedding storeæ•°æ®: {working_dir}")
        print("è¯·ç¡®ä¿ç›®å½•åŒ…å«ä»¥ä¸‹æ•°æ®ä¹‹ä¸€:")
        print(f"  - ChromaDBæ•°æ®: {chroma_chunk_path}")
        print(f"  - ä¼ ç»Ÿembedding storeæ•°æ®: {traditional_chunk_path}")
        return
    
    print(f"ğŸ“ ä½¿ç”¨å·¥ä½œç›®å½•: {working_dir}")
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = ChromaGraphVisualizer(working_dir)
    
    # åŠ è½½æ•°æ®
    if not visualizer.load_chroma_stores():
        return
    
    if not visualizer.load_graph():
        return
    
    # åŠ è½½OpenIEæ•°æ®ï¼ˆå¯é€‰ï¼‰
    visualizer.load_openie_data()
    
    # æ„å»ºNetworkXå›¾
    result = visualizer.build_networkx_graph()
    if result is None:
        return
    
    G, diagnostics = result
    
    # ç”Ÿæˆå¯è§†åŒ–
    data_source_prefix = "chroma" if (ChromaStore and isinstance(visualizer.chunk_store, ChromaStore)) else "hipporag"
    output_file = f"{data_source_prefix}_knowledge_graph_{os.path.basename(working_dir)}.html"
    visualizer.visualize_graph(G, output_file, diagnostics)
    
    data_source_type = "ChromaDB" if (ChromaStore and isinstance(visualizer.chunk_store, ChromaStore)) else "ä¼ ç»ŸEmbedding Store"
    print(f"\nğŸ‰ å®Œæˆï¼è¯·æ‰“å¼€ {output_file} æŸ¥çœ‹HippoRAGçŸ¥è¯†å›¾è°±")
    print(f"âœ¨ {data_source_type}å›¾è°±ç‰¹è‰²:")
    
    if ChromaStore and isinstance(visualizer.chunk_store, ChromaStore):
        print("   - ğŸ—„ï¸ åŸºäºChromaDBå‘é‡å­˜å‚¨")
        print("   - ğŸš€ é«˜æ•ˆçš„å‘é‡æ£€ç´¢å’Œç›¸ä¼¼åº¦è®¡ç®—")
        print("   - ğŸ“Š æŒä¹…åŒ–å­˜å‚¨ï¼Œé¿å…å†…å­˜çˆ†ç‚¸")
        print("   - ğŸ” æ”¯æŒè¯­ä¹‰æœç´¢å’Œç›¸ä¼¼åº¦æŸ¥è¯¢")
        print("   - âš¡ å¯æ‰©å±•åˆ°å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±")
    else:
        print("   - ğŸ“ åŸºäºä¼ ç»ŸEmbedding Store")
        print("   - ğŸ”„ è‡ªåŠ¨é€‚é…å…¼å®¹æ¨¡å¼")
        print("   - ğŸ“‹ Parquetæ ¼å¼å­˜å‚¨")
        print("   - âš¡ å¿«é€Ÿæ–‡ä»¶åŠ è½½")
        print("   - ğŸ”§ å‘åå…¼å®¹æ€§ä¿è¯")
    
    print("   - ğŸ¯ ä¿æŒä¸åŸå§‹å›¾è°±ç»“æ„çš„å®Œå…¨å…¼å®¹")

if __name__ == "__main__":
    main() 